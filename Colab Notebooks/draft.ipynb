{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0yYUMxY1Rn3B"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbCR1ZZxRsAR"},"outputs":[],"source":["%cd /content/gdrive/MyDrive/proj"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1651239784975,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"Zmi0ngRXSp82"},"outputs":[],"source":["import sys\n","sys.path.append('/content/gdrive/MyDrive/proj')"]},{"cell_type":"markdown","metadata":{"id":"EphuElvHmkNY"},"source":["## autoencoder "]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10929,"status":"ok","timestamp":1651239796496,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"uyYqtEypTLZm"},"outputs":[],"source":["from generate_model import Designed_model as GM\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import Input,Conv2D,MaxPooling2D,Flatten,Reshape,Dense, GaussianNoise,Lambda,Dropout\n","from keras.models import Model\n","from keras import regularizers\n","from keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Adam,SGD\n","from keras import backend as K\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","\n","class aphl(GM):\n","   def __init__(self,data_path):\n","      super(aphl, self).__init__(data_path)\n","   \n","   def build_model_and_get_data(self):\n","      if self.X_train == None : self.set_data() \n","      self.model = self.CNN_model_article(\n","         self.X_train[0].shape, \n","         self.y_train[0].shape[0])\n","      self.compile_model(self.model)\n","      return \n","   \n","   def train_gpu(self,epochs =20,batch_size=1024 ,compile = True): \n","       with tf.device(tf.test.gpu_device_name()):\n","          self.train(epochs=epochs, batch_size=batch_size)    \n","\n","   def train(self, epochs =20,batch_size=1024 ,compile = True, get_data_build =True):\n","      filepath = self.data_path\n","      if get_data_build : self.build_model_and_get_data()\n","      if compile : self.model = self.compile_model(self.model)\n","      history = self.model.fit(self.X_train,self.y_train,\n","              validation_data=(self.X_test, self.y_test),\n","              callbacks = [\n","                    keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n","                    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')],\n","              epochs=epochs, batch_size = batch_size, verbose=2 )\n","      \n","      return history\n","   \n","   \n","   def set_data(self , limit = None) -\u003e None:\n","      ##read from directory\n","      df = pd.read_pickle( self.data_path + \"RML2016.10a_dict.pkl\")\n","      d = pd.DataFrame(df.keys())\n","      snrs,mods = d[0].unique() , d[1].unique().tolist()\n","      if type (snrs[1]) == str : \n","         snrs,mods = d[1].unique() , d[0].unique().tolist()\n","      ## creat label \n","      X = []  \n","      label = []\n","      for key , value in df.items():\n","         label.extend([key]*value.shape[0])\n","         X.append(value)\n","      X = np.vstack(X)\n","      \n","      ## make one-hot label\n","      encoded = to_categorical([i for i in range(len(mods))]) \n","      \n","      # seperate data for train and test\n","      y = label \n","      \n","      if self.limit_check(limit,X):\n","            X = X[:limit]\n","            y = X[:limit] \n","\n","      X_mean = self.mean_data(X)\n","      self._mean = X_mean\n","      \n","      X_scale = self.scale(X)\n","      self._mean = X_scale\n","      \n","      X = self.transform(X,X_mean,X_scale)\n","\n","      X_train, X_test, y_train, y_test = train_test_split(\n","                                             X, y, test_size=0.4, random_state=42)\n","\n","      SNR_train = np.array([lbl[1] for lbl in y_train])\n","      y_train = np.array([self.mods_to_one_hot(mods,lbl[0],encoded) for lbl in y_train])\n","      y_test  = np.array([self.mods_to_one_hot(mods,lbl[0],encoded) for lbl in y_test])\n","\n","      sorted_arg_SNR_descended = SNR_train.argsort()[::-1]\n","      X_train = X_train[sorted_arg_SNR_descended]\n","      y_train = y_train[sorted_arg_SNR_descended]\n","\n","      X = X.astype('float64')\n","      y_train = y_train.astype('float64')\n","      y_test = y_test.astype('float64')\n","\n","      self.X_train , self.y_train, self.X_test, self.y_test = X_train ,y_train, X_test,y_test\n","   \n","   @staticmethod\n","   def compile_model(model):\n","      model.compile(\n","                    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n","                    loss= tf.keras.losses.CategoricalCrossentropy(),\n","                    metrics=[\"accuracy\"],\n","                )  \n","      return model\n","   \n","   @staticmethod\n","   def mods_to_one_hot (mods,inp,encoded):\n","     \n","    return encoded[mods.index(inp)]\n","\n","\n","   @staticmethod\n","   def complex_to_array(dataset):\n","      shape = dataset.shape\n","      temp = dataset.reshape([*shape,1])\n","      real_num , imag_num = np.real(temp) , np.imag(temp)\n","      array_form_of_complex = np.concatenate((real_num,imag_num),axis=len(shape))\n","      return array_form_of_complex \n","\n","   @staticmethod\n","   def transform(x,mean,scale):\n","     return (x-mean)/scale\n","\n","\n","   @staticmethod \n","   def mean_data(data):\n","    for i in range(len(data.shape)-2):\n","        if i == 0 : mean = np.mean(data,axis=0)\n","        mean = np.mean(mean,axis=0)   \n","    return mean\n","    \n","   @staticmethod \n","   def scale(data):\n","    max = np.max(np.abs(data))\n","    return  max \n","\n","   \n","   @staticmethod\n","   def CNN_model_article(input_shape = (2,128) , output_shape=11):\n","\n","    input_signal = Input(shape = input_shape)\n","    reshape = Reshape((2,128,1))(input_signal)\n","    conv_1 = Conv2D(128 , kernel_size = (2, 8), activation='relu' )(reshape)\n","    max_pool_1 = MaxPooling2D (pool_size=(1,2), strides = 2,data_format ='channels_last')(conv_1)\n","    \n","    conv_2 = Conv2D(64, kernel_size =(1, 16), activation='relu')(max_pool_1)\n","    max_pool_2 = MaxPooling2D (pool_size=(1,2), strides = 2)(conv_2)\n","\n","    flatten = Flatten()(max_pool_2)\n","\n","    dense_1 = Dense(128, activation='relu')(flatten)\n","    dense_2 = Dense(64, activation='relu')(dense_1)\n","    dense_3 = Dense(32, activation='relu')(dense_2)\n","    dense_4 = Dense(output_shape, activation='softmax')(dense_3)\n","\n","    return Model(inputs = input_signal, outputs = dense_4)  "]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1651239796497,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"bRyoPLL805FI"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"0Jbx_oVXmZNo"},"source":["## DeepMIMO"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4489,"status":"ok","timestamp":1651239800967,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"BgNyvWbKbNS_","outputId":"b30418de-9bd6-494a-a3fe-b812f3821ad7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: DeepMIMO in /usr/local/lib/python3.7/dist-packages (0.97)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from DeepMIMO) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from DeepMIMO) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from DeepMIMO) (1.4.1)\n"]}],"source":["!pip install DeepMIMO"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1651239800968,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"VGP5jhP4b1bp"},"outputs":[],"source":["import DeepMIMO\n","import numpy as np "]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2118,"status":"ok","timestamp":1651239803081,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"BSs05tlsb1P6"},"outputs":[],"source":["import DeepMIMO\n","import numpy as np \n","from generate_model import Designed_model as GM\n","from sklearn.model_selection import train_test_split\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","import plotly.graph_objects as go\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers , Input , Sequential \n","from tensorflow.keras.layers import Flatten , Dense , Reshape \n","import keras.backend as K\n","\n","\n","class Deep_MIMO(GM):\n","   def __init__(self,data_path ,active_BS = 64,\n","                                user_row_first = 1,\n","                                user_row_last = 10,\n","                                bandwidth = 0.05,\n","                                subcarriers = 64 ,\n","                                subcarriers_sampling = 1 ,\n","                                subcarriers_limit = 32,\n","                                num_paths = 1,\n","                                num_set_antenna_M = 8):\n","      super(Deep_MIMO, self).__init__(data_path)\n","      for i,(key,value) in enumerate(locals().items()):\n","          if i \u003c 2 : continue\n","          setattr(self,key,value)\n","\n","\n","   \n","   def build_model_and_get_data(self,MLP_DroupOut = True):\n","      if type (self.X_train) == type(None): self.set_data() \n","\n","      input_shape = self.X_train[0].shape\n","      output_shape = self.y_train[0].shape\n","\n","      if MLP_DroupOut : self.model = self.MlP_with_dropouts (input_shape , output_shape)\n","      else : self.model = self.MLP( input_shape , output_shape)\n","      self.compile_model (self.model, self.MSE_loss )\n","      \n","      return self.model\n","   \n","\n","   def train_gpu(self,epochs =17,batch_size=100 ,compile = True) -\u003e None: \n","       with tf.device(tf.test.gpu_device_name()):\n","          self.train(epochs=epochs, batch_size=batch_size)    \n","\n","   def train(self, epochs =17,batch_size=100 ,compile = True, get_data_build =True) -\u003e None:\n","      if get_data_build : self.build_model_and_get_data()\n","      if compile : self.model = self.compile_model (self.model, self.MSE_loss )\n","      history = self.model.fit(self.X_train,self.y_train,\n","              validation_data=(self.X_test, self.y_test),\n","              epochs=epochs, batch_size = batch_size, verbose=2 )\n","      return history\n","   \n","   \n","   def set_data(self , limit = None) -\u003e None:\n","\n","      dataset_up = self.get_MIMO_data('I1_2p4')\n","      dataset_down = self.get_MIMO_data('I1_2p4')\n","\n","      dataset_up = self.draw_necessary_inf_in_desired_shape (dataset_up)\n","      dataset_down = self.draw_necessary_inf_in_desired_shape (dataset_down)\n","\n","      data_up = self.complex_to_array(dataset_up)\n","      data_down = self.complex_to_array(dataset_down)\n","\n","      \n","      X_mean = self.mean_data(data_up)\n","      y_mean = self.mean_data(data_down)\n","      self._mean = X_mean\n","      \n","      X_scale = self.scale(data_up)\n","      y_scale = self.scale (data_down)\n","      self._scale = X_scale\n","      \n","      norm_data_up = self.transform(data_up,X_mean,X_scale)\n","      norm_data_down = self.transform(data_down,y_mean,y_scale)\n","\n","      X = np.array([u*self.make_mask(u,num_set_antenna_M = self.num_set_antenna_M) for u in norm_data_up[:]])\n","      y = norm_data_down\n","\n","      X = X.astype('float64')\n","      y = y.astype('float64')\n","      X_train, X_test, y_train, y_test = train_test_split(\n","                                              X, y, test_size=0.33, random_state=42)\n","      \n","      self.X_train , self.y_train, self.X_test, self.y_test = X_train ,y_train, X_test,y_test\n","\n","\n","   \n","   def compile_model(self, model, func ):\n","      model.compile(\n","                      optimizer=keras.optimizers.Adam(learning_rate=0.001),\n","                      loss= func,\n","                      metrics=[\"accuracy\"],\n","                  )  \n","      return model  \n","\n","   def model_eval (batch_size = 100):\n","      self.model.evaluate(self.X_test, self.y_test, batch_size=batch_size, verbose=2)\n","\n","   \n","   @staticmethod\n","   def draw_necessary_inf_in_desired_shape(data):\n","     #just using one RX antenna data and one TX antenna form all antennas information  \n","      data = [data[i]['user'][\"channel\"][:][:,0,0] for i in range(len(data))]\n","      data = np.array(data)\n","      print(data.shape)\n","      #changing the data into form of : [num_user,num_antena,num_subchannel]  \n","      data = np.transpose(data,axes=(1,0,2))\n","      return data\n","   \n","   @staticmethod\n","   def mods_to_one_hot (mods,inp,encoded):\n","     \n","    return encoded[mods.index(inp)]\n","   \n","   @staticmethod\n","   def MlP(input_shape,output_shape,\n","                      num_layers = [2**10,2**12,2**12,64*32*2],\n","                      k_r = None):\n","     \n","      num_last_layer = output_shape[0]*output_shape[1]*output_shape[2]\n","      input_signal = Input(shape = input_shape) \n","      flatten =Flatten()(input_signal)\n","      for i,num in enumerate(num_layers[1:-1]):\n","        if i==0: dense = Dense(num, activation=\"relu\" , kernel_regularizer = k_r)(flatten)\n","        dense = Dense(num, activation=\"relu\" , kernel_regularizer = k_r)(dense)\n","\n","      last_dense = Dense(num_last_layer, activation=None , kernel_regularizer = k_r)(dense)    \n","      out_put = Reshape(output_shape, input_shape=(last_dense))(last_dense)  \n","      return Model(inputs=input_signal,outputs=out_put) \n"," \n","\n","   @staticmethod\n","   def MlP_with_dropouts(input_shape,output_shape,\n","                      num_layers = [2**10,2**12,2**12,64*32*2],\n","                      dropout_rate=.1 ,\n","                      k_r = None):\n","     \n","      num_last_layer = output_shape[0]*output_shape[1]*output_shape[2]\n","      input_signal = Input(shape = input_shape) \n","      flatten =Flatten()(input_signal)\n","      for i,num in enumerate(num_layers[1:-1]):\n","        if i==0: dense = Dense(num, activation=\"relu\" , kernel_regularizer = k_r)(flatten)\n","        dense = Dense(num, activation=\"relu\" , kernel_regularizer = k_r)(dense)\n","        drop = Dropout(dropout_rate)(dense)\n","\n","      last_dense = Dense(num_last_layer, activation=None , kernel_regularizer = k_r)(drop)    \n","      out_put = Reshape(output_shape, input_shape=(num_last_layer))(last_dense)   \n","      return Model(inputs=input_signal,outputs=out_put) \n","\n","\n","   @staticmethod\n","   def complex_to_array(dataset):\n","      shape = dataset.shape\n","      temp = dataset.reshape([*shape,1])\n","      real_num , imag_num = np.real(temp) , np.imag(temp)\n","      array_form_of_complex = np.concatenate((real_num,imag_num),axis=len(shape))\n","      return array_form_of_complex \n","\n","   @staticmethod\n","   def transform(x,mean,scale):\n","     return (x-mean)/scale\n","\n","\n","   @staticmethod \n","   def mean_data(data):\n","    for i in range(len(data.shape)-2):\n","        if i == 0 : mean = np.mean(data,axis=0)\n","        mean = np.mean(mean,axis=0)   \n","    return mean\n","    \n","   @staticmethod \n","   def scale(data):\n","    max = np.max(np.abs(data))\n","    return  max \n","\n","\n","   @staticmethod  \n","   def make_mask (dataset , inp_mask = None , num_set_antenna_M = 8):\n","    num_ant ,num_subchannel = dataset.shape[0:2];\n","    mask = np.zeros(dataset.shape);\n","\n","    if inp_mask == None : \n","        antena_selected_mask = np.random.choice(num_ant,\n","                                                num_set_antenna_M,\n","                                                replace=False)\n","        \n","    else: antena_selected_mask = np.where( mask == 1)\n","    mask[ antena_selected_mask] =  1\n","    return mask\n","\n","   @staticmethod\n","   def plot_imshow(*atr):\n","    fig = make_subplots(rows=len(atr), cols=2)\n","    for j in range(len(atr)):\n","        for i in range(2):\n","            fig.add_trace(px.imshow(atr[j][:,:,i]).data[0], row=j+1,col=i+1)\n","    fig.show()\n","\n","\n","   @staticmethod\n","   def NMSE_loss (y_true , y_pred):\n","    return tf.cast(K.mean(K.square(y_pred - y_true)/ (2*K.square(y_true))),tf.float64)\n","\n","   @staticmethod\n","   def MSE_loss (y_true , y_pred):\n","    return K.mean(K.square(y_pred-y_true))\n","\n","  \n","   def get_MIMO_data(self,scenario ) :\n","\n","     # Load the default parameters\n","      parameters = DeepMIMO.default_params()\n","      # Set scenario name\n","      parameters['scenario'] = scenario\n","      # Set the main folder containing extracted scenarios\n","      parameters['dataset_folder'] = r''+ self.data_path\n","      # To activate the basestations from 1 tiil 64, set\n","      # each antena has 8 RX \n","      parameters['active_BS'] = np.arange(1,self.active_BS + 1)\n","      # To activate the user rows 1-5 \n","      # according to practical examination i2_2p4 has 201 in each column \n","      parameters['user_row_first'] = self.user_row_first\n","      parameters['user_row_last'] = self.user_row_last\n","\n","\n","      # Number of BS antennas in (x, y, z)\n","      parameters['bs_antenna']['shape'] = np.array([1, 1, 1])\n","      # To generate channels at 0.02 GHz  bandwidth, set\n","      parameters['OFDM']['bandwidth'] = self.bandwidth\n","      # To generate OFDM channels with 64 subcarriers, set\n","      parameters['OFDM']['subcarriers'] = self.subcarriers\n","      # To sample first 16 subcarriers by every spacing between each, set\n","      parameters['OFDM']['subcarriers_sampling'] = self.subcarriers_sampling\n","      #according to the picture in the article we chioced 32 instead of 16, that was mentioned in TABLE 1\n","      parameters['OFDM']['subcarriers_limit'] = self.subcarriers_limit\n","      # To only include 1 strongest paths in the channel computation, set\n","      parameters['num_paths'] = self.num_paths\n","\n","      #Note: Since this scenario consists \n","      #of only one access point, please set “enable_BS2BSchannels”\n","      #(MATLAB) or “enable_BS2BS” (Python) to zero in the DeepMIMO \n","      #generation parameters. \n","      parameters['enable_BS2BS'] = 0\n","\n","      # Generate data\n","      dataset = DeepMIMO.generate_data(parameters)\n","\n","      #delete unnccessary info\n","        #find keys in dictionary which is unneccessary\n","      dataset_keys = list(dataset[0].keys())\n","      dataset_user_keys = list(dataset[0]['user'].keys())\n","        #delte those keys \n","      for i in range(len(dataset)):\n","          for del_key in [key for key in dataset_keys if key is not 'user']:\n","              del dataset[i][del_key]\n","          for del_key in [key for key in dataset_user_keys if key is not 'channel']:\n","              del dataset[i]['user'][del_key]\n","\n","      return dataset"]},{"cell_type":"markdown","metadata":{"id":"RyAHNuhfya3H"},"source":["mim.y_train.shape"]},{"cell_type":"markdown","metadata":{"id":"asEOyxAPmREy"},"source":["## PE"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1651239803081,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"NmZNBhHrIb5K","outputId":"d37b68d7-a593-48dd-b8ef-77d07c380b85"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/MyDrive\n"]}],"source":["%cd /content/gdrive/MyDrive"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":716,"status":"ok","timestamp":1651239803793,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"TexMEbHYIn0N"},"outputs":[],"source":["from generate_model import Designed_model as GM\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import Input,Conv2D,MaxPooling2D,Flatten,Reshape,Dense, GaussianNoise,Lambda,Dropout\n","from keras.models import Model\n","from keras import regularizers\n","from keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Adam,SGD\n","from keras import backend as K\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Model\n","from tensorflow.keras import layers , Input\n","from tensorflow.keras.layers import Dense , LSTM ,Dropout , Activation ,Reshape\n","\n","class PE(GM):\n","   def __init__(self,data_path):\n","      super(PE, self).__init__(data_path)\n","\n","\n","\n","   def build_model_and_get_data(self):\n","\n","      if type(self.X_train) == type(None) : self.set_data() \n","\n","      self.model = self.RNN_model ( self.X_train[0].shape, \n","                                    self.y_train[0].shape[0])\n","      \n","      self.compile_model(self.model)\n","      \n","      return \n","   \n","\n","\n","   def train_gpu(self,epochs =20,batch_size=1024 ,compile = True): \n","       with tf.device(tf.test.gpu_device_name()):\n","          self.train(epochs=epochs, batch_size=batch_size)    \n","\n","   def train(self, epochs =20,batch_size=1024 ,compile = True, normalize = True , get_data_build =True):\n","\n","      if get_data_build : self.build_model_and_get_data()\n","      if compile : self.model = self.compile_model(self.model)\n","      if normalize : self.norm ()\n","\n","      history = self.model.fit(self.X_train,self.y_train,\n","              validation_data=(self.X_test, self.y_test),\n","              epochs=epochs, batch_size = batch_size, verbose=2 )\n","      \n","      return history\n","   \n","   def norm(self):\n","\n","      self._mean = self.mean_data(np.array([self.mean_data(self.X_train),\n","                                          self.mean_data(self.X_test)]))\n","\n","      self._scale = max(self.scale(self.X_train),\n","                        self.scale(self.X_test))\n","      \n","      for value in [self.X_train,self.X_test]:\n","        value = self.transform(value,self._mean,self._scale)\n","\n","   \n","   \n","   \n","   def set_data(self , limit = 500000 , execute = False)  -\u003e None:\n","\n","      train_batch_size = 128\n","      train_snr = np.arange(20, -4, -2)\n","      test_snr = np.arange(0, 6.5, 0.5)\n","      train_ratio = np.array([0.4, 0.6, 0.8, 1.0])\n","      epoch_setting = np.array([10**1, 10**2, 10**3, 10**4, 10**5])\n","\n","      if  execute : \n","        os.mkdir(self.data_path)\n","        exec(open(self.data_path + 'deep-neural-network-decoder/RNN/noise/K_16_N_32/\"train_data_10^6\"/get_data.py'))\n","\n","        ##### execute train ##########\n","        X_train, y_train = self.get_data_by_ratio (train_ratio = 0.8 ,\n","                                                   snr = train_snr,status = \"train\",\n","                                                   limit= limit,\n","                                                   data_path = self.data_path + \"data/\")    \n","        \n","        np.savez_compressed( self.data_path + 'train_shape', X_train_shape= np.array(X_train.shape) , y_train_shape = np.array(y_train.shape))\n","        print(\"train data shape saved\")\n","        np.savez_compressed( self.data_path + 'train', X_train= X_train.reshape(-1,1), y_train = y_train.reshape(-1,1))\n","        print(\"train data saved\")\n","        del X_train, y_train\n","\n","        ##### execute test ##########\n","        X_test, y_test = self.get_data_by_ratio (train_ratio = 0.8 ,\n","                                                   snr = train_snr,status = \"train\",\n","                                                   limit= int(limit*0.2),\n","                                                   data_path = self.data_path + \"data/\",\n","                                                 ) \n","\n","        np.savez_compressed( self.data_path + 'test_shape', X_test_shape= np.array(X_test.shape) , y_test_shape = np.array(y_test.shape))\n","        print(\"test data shape saved\")\n","        np.savez_compressed( self.data_path + 'test', X_test= X_test.reshape(-1,1), y_test = y_test.reshape(-1,1))\n","        print(\"test data saved\")\n","        del X_test, y_test\n","\n","      ###### Load data from npy saved based on exceuting data earlier ########\n","      print(\"Loading Train Data...\")\n","      loaded = np.load( self.data_path + 'train.npz')\n","      loaded_shape = np.load( self.data_path + 'train_shape.npz')\n","      X_train_shape , y_train_shape = loaded_shape[\"X_train_shape\"] , loaded_shape[\"y_train_shape\"] \n","      self.X_train , self.y_train = loaded[\"X_train\"].reshape(X_train_shape) , loaded[\"y_train\"].reshape(y_train_shape)\n","\n","\n","      print(\"Loading test Data...\")\n","      loaded = np.load( self.data_path + 'test.npz')\n","      loaded_shape = np.load( self.data_path + 'test_shape.npz')\n","      X_test_shape , y_test_shape = loaded_shape[\"X_test_shape\"] , loaded_shape[\"y_test_shape\"] \n","      self.X_test , self.y_test = loaded[\"X_test\"].reshape(X_test_shape) , loaded[\"y_test\"].reshape(y_test_shape)\n","      print(\"loading is finished \")\n","      del loaded\n","      # make the data independent of snr \n","      self.X_train , self.y_train = self.X_train.reshape(-1,X_train_shape[-1]) , self.y_train.reshape(-1,y_train_shape[-1])\n","      self.X_test , self.y_test = self.X_test.reshape(-1,X_test_shape[-1]) , self.y_test.reshape(-1,y_test_shape[-1])\n","      \n","   \n","   \n","   @staticmethod\n","   def transform(x,mean,scale):\n","     return (x-mean)/scale\n","\n","   @staticmethod\n","   def mean_data(data):\n","      mean = np.mean(data,axis=0)\n","      for i in range(1,len(data.shape)-2):\n","        mean = np.mean(mean,axis=0)   \n","      return mean\n","\n","   @staticmethod \n","   def scale(data):\n","      max = np.max(np.abs(data))\n","      return max \n","\n","\n","\n","   @staticmethod\n","   def compile_model(model):\n","      model.compile(\n","                    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n","                    loss= tf.keras.losses.MeanSquaredError(),\n","                    metrics=[\"accuracy\"],\n","                )\n","      return model\n","   \n","\n","   @staticmethod\n","   def RNN_model (input_shape,output_shape, Dropout_rate = 0.1) :\n","  \n","    input_signal = Input(shape = input_shape)\n","    reshape = Reshape((*input_shape,1))(input_signal)\n","    lstm = LSTM(256,\n","                dropout = Dropout_rate,\n","                kernel_initializer= tf.keras.initializers.GlorotNormal(seed=None))(reshape)\n","\n","    output = Dense(output_shape,\"sigmoid\")(lstm)\n","    \n","    return Model(inputs = input_signal, outputs = output)\n","\n","\n","    @staticmethod\n","    def get_data_by_ratio (train_ratio, snr, status = \"train\", limit= None,\n","                      data_path = \"/content/project_3/PE/data/\"):\n","      \n","      if status == \"train\" : data = sio.loadmat(data_path+\"ratio_0.4_train_snr_8dB\")\n","      elif status == \"test\" : data = sio.loadmat(data_path+'test_snr_0.0dB')\n","      x_shape = data['x_'+status].shape\n","      y_shape = data['y_'+status].shape\n","      x_shape = [min(x_shape[0],limit) , x_shape[1]]\n","      y_shape = [min(y_shape[0],limit) , y_shape[1]]\n","\n","      x = np.zeros([len(snr),*x_shape])\n","      y = np.zeros([len(snr),*y_shape])\n","      print(x_shape)\n","      del data\n","    \n","      for i, tr_snr in enumerate(snr):\n","          print(\"added {} SNR\".format(tr_snr))\n","          if status == \"train\" : filename = 'ratio_' + str(train_ratio) + '_train_snr_' + str(tr_snr) + 'dB'\n","          elif status == \"test\" : filename = 'test_snr_' + str(tr_snr) + 'dB'\n","          data = sio.loadmat(data_path+filename)\n","          x[i,:,:] = data['x_'+status][:limit,:]\n","          y[i,:,:] = data['y_'+status][:limit,:]\n","\n","      return [x , y]\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tOpMgnZuC4d-"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"-FTv1lsDmMnY"},"source":["## MIST"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1651239803793,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"DVM8vIUNOqe5"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","from keras.layers.core import Dense\n","from keras.layers import Conv1D,Flatten, BatchNormalization, Input\n","from keras import backend as K\n","from keras.models import Model\n","from keras.callbacks import ModelCheckpoint\n","\n","\n","from generate_model import Designed_model as GM\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import Input,Conv2D,MaxPooling2D,Flatten,Reshape,Dense, GaussianNoise,Lambda,Dropout\n","from keras.models import Model\n","from keras import regularizers\n","from keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Adam,SGD\n","from keras import backend as K\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","\n","class MIST():\n","   def __init__(self, k = 500 ,\n","                      rate = 1/2 ,\n","                      SNR_dB_start_Eb = -1 ,\n","                      SNR_dB_stop_Eb = 7 ,\n","                      SNR_points = 9 ,\n","                      n_samples = 10 , \n","                      train_batch = 1024,\n","                      test_batch = 1024,\n","                      g1=[1,1,1],\n","                      g2=[1,0,1]):\n","     \n","      #dataword length  =\u003e k  \n","      #encoder rate is used =\u003e rate \n","      self.N = int(k/rate)  #codeword length\n","      for i,(key,value) in enumerate(locals().items()):\n","          if i \u003c 2 : continue\n","          setattr(self,key,value)\n","   \n","   def build_model(self):\n","      self.model = self.MIST_model(input_shape=(self.N,1),output_shape=(self.k)) \n","      return self.model\n","\n","   def build_model_and_get_data(self):\n","      self.build_model()\n","      self.X_train , self.y_train = self.get_data()\n","      self.compile_model() \n","   \n","   def train_gpu(self,epochs = 200 , batch_size=1024 ,compile = True): \n","       with tf.device(tf.test.gpu_device_name()):\n","          self.train(epochs=epochs, batch_size=batch_size)    \n"," \n","   def train(self, epochs = 200, batch_size=1024 ,MIST_train = True ,compile = True, get_data_build =True):\n","      self.n_samples = int(epochs/self.SNR_points) \n","      self.build_model()\n","      self.compile_model()\n","      \n","      if MIST_train : \n","        history = self.MIST_train_idea()\n","\n","      \n","      else :\n","          X_train , y_train = self.get_data()\n","          history = self.model.fit(X_train,y_train,\n","                epochs = 1, batch_size = batch_size, verbose=2 )\n","      return history\n","   \n","   \n","   def get_data(self , limit = None ) :\n","      sigmas = self. get_sigmas(self.SNR_dB_start_Eb,\n","                                self.SNR_dB_stop_Eb,\n","                                self.SNR_points,\n","                                self.k,\n","                                self.N)\n","      \n","      X,y = self.generate_uncode_noisy_signal(sigmas)\n","      return  [X,y]\n","\n","  \n","   \n","\n","   \n","   def generate_uncode_noisy_signal(self,sigmas):\n","     \n","     X = np.zeros([len(sigmas),self.n_samples,self.train_batch,self.N,1])\n","     y = np.zeros([len(sigmas),self.n_samples,self.train_batch,self.k])\n","     for i in range(0,self.n_samples): \n","        for ii in range(0,len(sigmas)):\n","            #Generating dataword and codeword\n","            uncoded = np.random.randint(0,2,size=(self.train_batch,self.k))\n","            encoded = np.zeros([uncoded.shape[0], self.N])\n","            # memory 2 convolutional encoder used g1, g2\n","            for iii in range(0,self.train_batch): \n","                encoded[iii,:] = self.convenc (uncoded[iii,:],self.g1,self.g2,self.k)\n","            #Modulate\n","            signal = 2*encoded - 1\n","            #Adding noise\n","            noisy_signal = signal + np.random.normal(0, sigmas[i], size= np.shape(signal))\n","            ## train \n","            X[i,ii,:,:,0] = noisy_signal\n","            y[i,ii,:,:] = uncoded\n","      \n","     return [ X.reshape(-1,self.N) , y.reshape(-1,self.k)]\n","\n","\n","\n","\n","   def MIST_train_idea(self , limit = None ):\n","     sigmas = self.get_sigmas(self.SNR_dB_start_Eb,\n","                                self.SNR_dB_stop_Eb,\n","                                self.SNR_points,\n","                                self.k,\n","                                self.N)\n","     history = [] \n","     for i in range(0,len(sigmas)):\n","       print(\"SNR  {}/{}\".format(i,len(sigmas)))\n","       for ii in range(0,self.n_samples):\n","            #Generating dataword and codeword\n","            uncoded = np.random.randint(0,2,size=(self.train_batch,self.k))\n","            encoded = np.zeros([uncoded.shape[0], self.N])\n","            # memory 2 convolutional encoder used g1, g2\n","            for iii in range(0,self.train_batch): \n","                encoded[iii,:] = self.convenc(uncoded[iii,:],self.g1,self.g2,self.k)\n","            #Modulate\n","            signal = 2*encoded - 1\n","            #Adding noise\n","            noisy_signal = signal + np.random.normal(0, sigmas[i], size= np.shape(signal))\n","            ## train l\n","            print(\"epoch  {}/{}\".format(ii,self.n_samples))\n","            x_train = np.expand_dims(noisy_signal,axis=2)\n","            history.append( self.model.fit(x_train,uncoded, epochs = 1, batch_size = self.train_batch , verbose=2))\n","\n","     return history \n","\n","   def compile_model(self, optimizer = 'adam',loss = 'mse' ):            \n","      self.model.compile(optimizer=optimizer, loss=loss, metrics=[self.ber]) \n","      return self.model\n","\n","   \n","   @staticmethod\n","   def get_sigmas(SNR_dB_start_Eb,SNR_dB_stop_Eb,SNR_points,k,N):\n","      \n","      SNR_dB_start_Es = SNR_dB_start_Eb + (10*np.log10(1.0*k/N))\n","      SNR_dB_stop_Es = SNR_dB_stop_Eb + (10*np.log10(1.0*k/N))\n","      SNR_range=np.linspace(SNR_dB_start_Es, SNR_dB_stop_Es, SNR_points)\n","      return np.sqrt(1/(2*10**(SNR_range/10)))\n","   \n","   \n","   @staticmethod\n","   def convenc(data,g1,g2,k):\n","    enc_msg = np.zeros([2*k])\n","    enc_msg[0::2] = (np.convolve(data,g1)%2)[0:k]\n","    enc_msg[1::2] = (np.convolve(data,g2)%2)[0:k]\n","    return enc_msg   \n","\n","    \n","   @staticmethod\n","   def ber(y_true, y_pred):\n","     return  K.mean(K.cast(K.not_equal(y_true, K.round(y_pred)),dtype='float32'))\n","   \n","   @staticmethod\n","   def MIST_model(input_shape , output_shape):  \n","      input_batch=Input(shape=input_shape)\n","      conv1 = Conv1D(10, 24, activation='relu',padding='same')(input_batch)\n","      batch_norm1=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv1)\n","      conv2 = Conv1D(50, 24, activation='relu',padding='same')(batch_norm1)\n","      batch_norm2=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv2)\n","      conv3 = Conv1D(50, 24, activation='relu',padding='same')(batch_norm2)\n","      batch_norm3=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(conv3)\n","      flatten = Flatten()(batch_norm3)\n","      msg_out = Dense(output_shape, activation='sigmoid')(flatten)\n","\n","      return Model(inputs=input_batch,outputs=msg_out)\n","  "]},{"cell_type":"markdown","metadata":{"id":"HYjo36eVmw6M"},"source":["# test"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1651239803794,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"15Y9c-l9Xo5-"},"outputs":[],"source":["a = aphl(\"/content/gdrive/MyDrive/RML2016.10a/\")"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1651239803794,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"l3LKeXxMmBjR"},"outputs":[],"source":["p = PE(\"/content/gdrive/MyDrive/project_3/PE\")"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1651239803794,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"TLOvVSvCm5GM"},"outputs":[],"source":["mim = Deep_MIMO('/content/gdrive/MyDrive/DeepMIMO2',active_BS=20,user_row_last=2)"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":368,"status":"ok","timestamp":1651227546458,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"LHJvEXYqnE-9"},"outputs":[],"source":["mis = MIST()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122386,"status":"ok","timestamp":1651235545170,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"IovzgvE_nJFR","outputId":"28277139-e9ee-474e-de52-7cfc846f63d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 17s - loss: 2.0855 - accuracy: 0.2224 - val_loss: 1.9215 - val_accuracy: 0.2810 - 17s/epoch - 134ms/step\n","Epoch 2/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.8183 - accuracy: 0.3151 - val_loss: 1.7424 - val_accuracy: 0.3368 - 5s/epoch - 40ms/step\n","Epoch 3/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 4s - loss: 1.6929 - accuracy: 0.3580 - val_loss: 1.6548 - val_accuracy: 0.3715 - 4s/epoch - 35ms/step\n","Epoch 4/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.6240 - accuracy: 0.3801 - val_loss: 1.6185 - val_accuracy: 0.3834 - 5s/epoch - 41ms/step\n","Epoch 5/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.5608 - accuracy: 0.3959 - val_loss: 1.5374 - val_accuracy: 0.3968 - 5s/epoch - 35ms/step\n","Epoch 6/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 4s - loss: 1.4964 - accuracy: 0.4111 - val_loss: 1.4744 - val_accuracy: 0.4056 - 4s/epoch - 35ms/step\n","Epoch 7/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.4561 - accuracy: 0.4174 - val_loss: 1.4699 - val_accuracy: 0.4174 - 5s/epoch - 40ms/step\n","Epoch 8/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.4351 - accuracy: 0.4230 - val_loss: 1.4331 - val_accuracy: 0.4224 - 5s/epoch - 42ms/step\n","Epoch 9/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.4227 - accuracy: 0.4293 - val_loss: 1.4244 - val_accuracy: 0.4223 - 5s/epoch - 35ms/step\n","Epoch 10/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.4047 - accuracy: 0.4410 - val_loss: 1.3959 - val_accuracy: 0.4497 - 5s/epoch - 40ms/step\n","Epoch 11/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.3675 - accuracy: 0.4656 - val_loss: 1.3636 - val_accuracy: 0.4676 - 5s/epoch - 40ms/step\n","Epoch 12/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.3349 - accuracy: 0.4824 - val_loss: 1.3285 - val_accuracy: 0.4805 - 5s/epoch - 35ms/step\n","Epoch 13/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.2967 - accuracy: 0.4978 - val_loss: 1.3012 - val_accuracy: 0.4940 - 5s/epoch - 42ms/step\n","Epoch 14/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.2749 - accuracy: 0.5058 - val_loss: 1.2704 - val_accuracy: 0.5013 - 5s/epoch - 40ms/step\n","Epoch 15/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.2487 - accuracy: 0.5132 - val_loss: 1.2679 - val_accuracy: 0.5034 - 5s/epoch - 40ms/step\n","Epoch 16/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.2318 - accuracy: 0.5200 - val_loss: 1.2504 - val_accuracy: 0.5081 - 5s/epoch - 39ms/step\n","Epoch 17/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.2262 - accuracy: 0.5216 - val_loss: 1.2378 - val_accuracy: 0.5157 - 5s/epoch - 37ms/step\n","Epoch 18/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.2117 - accuracy: 0.5250 - val_loss: 1.2269 - val_accuracy: 0.5210 - 5s/epoch - 40ms/step\n","Epoch 19/20\n","129/129 - 4s - loss: 1.2042 - accuracy: 0.5290 - val_loss: 1.2582 - val_accuracy: 0.5008 - 4s/epoch - 29ms/step\n","Epoch 20/20\n","INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/RML2016.10a/assets\n","129/129 - 5s - loss: 1.2003 - accuracy: 0.5297 - val_loss: 1.2209 - val_accuracy: 0.5234 - 5s/epoch - 36ms/step\n"]}],"source":["#a.train_gpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1Vxa9XtnPy6"},"outputs":[],"source":["#mim.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHuM7pnokGZi"},"outputs":[],"source":["#p.train()"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":8887,"status":"ok","timestamp":1651239814704,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"_SCLgSJgnM8L"},"outputs":[],"source":["a.build_model_and_get_data()"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":459,"status":"ok","timestamp":1651240115447,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"DCoKZOq2qgKA"},"outputs":[],"source":["def show (m):\n","  print('train shape : {}'.format(m.X_train.shape))\n","  print('train shape : {}'.format(m._mean))\n","  print(\"model {}\".format(m.model.summary()))  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Br5K-OvS9hQE"},"outputs":[],"source":["!pip3 install foolbox==3.1.1\n","# !pip3 install git+https://github.com/bethgelab/foolbox.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QZyJfM89noO"},"outputs":[],"source":["!pip3 install --upgrade tensorflow"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":438,"status":"ok","timestamp":1651240124854,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"BYtv9b2glKxW","outputId":"89757e3d-8577-4032-dad9-af463f30d6b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["train shape : (132000, 2, 128)\n","train shape : 0.1642291247844696\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 2, 128)]          0         \n","                                                                 \n"," reshape (Reshape)           (None, 2, 128, 1)         0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 1, 121, 128)       2176      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 1, 60, 128)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 1, 45, 64)         131136    \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 1, 22, 64)        0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 1408)              0         \n","                                                                 \n"," dense (Dense)               (None, 128)               180352    \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                2080      \n","                                                                 \n"," dense_3 (Dense)             (None, 11)                363       \n","                                                                 \n","=================================================================\n","Total params: 324,363\n","Trainable params: 324,363\n","Non-trainable params: 0\n","_________________________________________________________________\n","model None\n"]}],"source":["show(a)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5737,"status":"ok","timestamp":1651240270657,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"TTXsl6jLC8-d","outputId":"b0470e33-0b1c-47b8-ee56-077d3ed4caf3"},"outputs":[{"data":{"text/plain":["0.875"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["import foolbox as fb\n","model = tf.keras.applications.ResNet50(weights=\"imagenet\")\n","preprocessing = dict(flip_axis=-1, mean=[103.939, 116.779, 123.68])\n","bounds = (0, 255)\n","fmodel = fb.TensorFlowModel(model, bounds=bounds, preprocessing=preprocessing)\n","images, labels = fb.utils.samples(fmodel, dataset='imagenet', batchsize=16)\n","fb.utils.accuracy(fmodel, images, labels)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":386,"status":"ok","timestamp":1651240324801,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"zfoeZuP_kdWg","outputId":"adbd2f19-ef52-4ba4-8399-f34221631681"},"outputs":[{"data":{"text/plain":["TensorShape([16, 224, 224, 3])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["images.shape"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":454},"executionInfo":{"elapsed":4,"status":"error","timestamp":1651240558184,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"BOq-oCQt_cUD"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-25-66524db406f7\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorFlowModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 6\u001b[0;31m \u001b[0mfb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#attack = fb.attacks.LinfDeepFoolAttack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#raw, clipped, is_adv = attack(fmodel, mim.X_train, mim.y_train, epsilons=0.03)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'inpu' is not defined"]}],"source":["import foolbox as fb\n","model = a.model\n","preprocessing = dict()\n","bounds = (np.min(a.X_train), np.max(a.X_train))\n","fmodel = fb.TensorFlowModel(a.model, bounds=bounds,preprocessing=preprocessing)\n","fb.utils.accuracy(fmodel, inpu[:17], a.y_train[:17])\n","#attack = fb.attacks.LinfDeepFoolAttack()\n","#raw, clipped, is_adv = attack(fmodel, mim.X_train, mim.y_train, epsilons=0.03)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":411,"status":"ok","timestamp":1651235976281,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"O1J8M3XjUji0"},"outputs":[],"source":["i = tf.convert_to_tensor(a.X_train, np.float64)"]},{"cell_type":"code","execution_count":98,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3701,"status":"ok","timestamp":1651231285212,"user":{"displayName":"parsa job","userId":"13704868626815364480"},"user_tz":-270},"id":"ElKe9lXlDZ2s","outputId":"6a8599e8-6d7b-4bc9-ea65-bb3958f6c1a3"},"outputs":[{"data":{"text/plain":["\u003ckeras.engine.functional.Functional at 0x7f6009ddf290\u003e"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["model = tf.keras.applications.ResNet50(weights=\"imagenet\")\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJ27EOaP_-VM"},"outputs":[],"source":[""]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPc9t8YuJtf9uy2UIMxtE26","collapsed_sections":[],"name":"draft.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}